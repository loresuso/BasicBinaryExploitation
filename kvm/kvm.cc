#include <sys/types.h>
#include <sys/stat.h>
#include <sys/ioctl.h>
#include <fcntl.h>
#include <iostream>
#include <cerrno>
#include <cstring>
#include <sys/mman.h>
#include <unistd.h>
#include <elf.h>
#include <stdio.h>

using namespace std;

/* We are going to use the linux kvm API to crate a simple virtual machine and
 * execute some code inside it.
 *
 * The virtual machine is going to have just one CPU and a physical memory
 * consisting of just two pages
 */

/* First, we need to include the kvm.h file (which you can usually found in
 * /usr/include/linux/kvm.h).  The file contains the definitions of all the
 * constants and data structures that we are going to use, and it is the source
 * you should look at for the names of the fields and so no.
 *
 * Note: IA32/64 specific data structures (such as kvm_regs) are defined in
 * /usr/include/asm/kvm.h, included by this one.
 */
#include <linux/kvm.h>

/* the following array will become the other page of memory for the
 * machine. The code above uses it to contain the stack.
 */
static unsigned long const KiB = 1024UL;
static unsigned long const MiB = 1024*KiB;
static unsigned long const GUESTMEMSZ = 16*MiB;
unsigned char guestmem[GUESTMEMSZ] __attribute__ ((aligned(4096))) = {};

// defined after main
bool load_elf(int vcu_fd, const char *fname);
void emulate_hd(kvm_run *kr);
void emulate_serial(kvm_run *kr);
void foo(kvm_run *kr);
int main(int argc, char *argv[])
{
	if (argc != 2) {
		cerr << "missing file name" << endl;
		return 1;
	}

	/* the first thing to do is to open the /dev/kvm pseudo-device,
	 * obtaining a file descriptor.
	 */
	int kvm_fd = open("/dev/kvm", O_RDWR);
	if (kvm_fd < 0) {
		/* as usual, a negative value means error */
		cerr << "/dev/kvm: " << strerror(errno) << endl;
		return 1;
	}

	/* we interact with our kvm_fd file descriptor using ioctl()s.  There
	 * are several of them, but the most important here is the one that
	 * allows us to create a new virtual machine.  The ioctl() returns us a
	 * new file descriptor, which we can then use to interact with the vm.
	 */
	int vm_fd = ioctl(kvm_fd, KVM_CREATE_VM, 0);
	if (vm_fd < 0) {
		cerr << "create vm: " << strerror(errno) << endl;
		return 1;
	}

	/* initially, the vm has no resources: no memory, no cpus.  Here we add
	 * the (guest) physical memory, using the 'code' and 'data' arrays that
	 * we have defined above.  To add memory to the machine, we need to
	 * fill a 'kvm_userspace_memory_region' structure and pass it to the
	 * vm_fd using an ioctl().  The virtual machine has several 'slots'
	 * where we can add physical memory. The slot we want to fill (or
	 * replace) is the first field in the structure.  Following the slot
	 * number, we can specify some flags (e.g., to say that this memory is
	 * read only, perhaps to emulate a ROM). The remaining fields should be
	 * obvious.
	 */

	/* This is the descriptor for the 'data' page.  We want to put this
	 * page at guest physical address 0
	 */
	kvm_userspace_memory_region mrd = {
		0,					// slot
		0,					// no flags,
		0,					// guest physical addr
		GUESTMEMSZ,				// memory size
		reinterpret_cast<__u64>(guestmem)	// userspace addr
	};
	/* now we can add the memory to the vm */
	if (ioctl(vm_fd, KVM_SET_USER_MEMORY_REGION, &mrd) < 0) {
		cerr << "set memory (data): " << strerror(errno) << endl;
		return 1;
	}
	/* note that the memory is shared between us and the vm.  Whatever we
	 * write in the 'data' array above will be seen by the vm and,
	 * vice-versa, whatever the vm writes in its first "physical" page we
	 * can read in the in the 'data' array. We can even do this
	 * concurrently, if we use several threads.
	 */

	/* now we add a virtual cpu (vcpu) to our machine. We obtain yet
	 * another open file descriptor, which we can use to interact with the
	 * vcpu. Note that we can have several vcpus, to emulate a
	 * multi-processor machine.
	 */
	int vcpu_fd = ioctl(vm_fd, KVM_CREATE_VCPU, 0);
	if (vcpu_fd < 0) {
		cerr << "create vcpu: " << strerror(errno) << endl;
		return 1;
	}

	/* the exchange of information between us and the vcpu is via a
	 * 'kvm_run' data structure in shared memory, one for each vpcu. To
	 * obtain a pointer to this data structure we need to mmap() the
	 * vcpu_fd file descriptor that we obtained above. First, we need to
	 * know the size of the data structure, which we can obtain with the
	 * following ioctl() on the original kvm_fd (the one we obtained from
	 * the open("/dev/kvm")).
	 */
	long mmap_size = ioctl(kvm_fd, KVM_GET_VCPU_MMAP_SIZE, 0);
	if (mmap_size < 0) {
		cerr << "get mmap size: " << strerror(errno) << endl;
		return 1;
	}

	/* and now the mmap() */
	kvm_run *kr = static_cast<kvm_run *>(mmap(
			/* let the kernel  choose the address */
			NULL,
			/* the size we obtained above */
			mmap_size,
			/* we want to both read and write */
			PROT_READ|PROT_WRITE,
			/* this is a shared mapping. A private mapping
			 * would cause our writes to go into the swap area.
			 */
			MAP_SHARED,
			/* finally, the file descriptor we want to map */
			vcpu_fd,
			/* the 'offset' must be 0 */
			0
		));
	if (kr == MAP_FAILED) {
		cerr << "mmap: " << strerror(errno) << endl;
		return 1;
	}

	/* load the initial VM state (CPU and memory) */
	if (!load_elf(vcpu_fd, argv[1]))
		return 1;

	/* we are finally ready to start the machine, by issuing the KVM_RUN
	 * ioctl() on the vcpu_fd. While the machine is running our process is
	 * 'inside' the ioctl(). When the machine exits (for whatever reason),
	 * the ioctl() returns. We can then read the reason for the exit in the
	 * kvm_run structure that we mmap()ed above, take the appropriate
	 * action (e.g., emulate I/O) and re-enter the vm, by issuing another
	 * KVM_RUN ioctl().
	 */
	bool running = true;
	while (running) {
		if (ioctl(vcpu_fd, KVM_RUN, 0) < 0) {
			cerr << "run: " << strerror(errno) << endl;
			return 1;
		}

		switch (kr->exit_reason) {
		case KVM_EXIT_IO:
			/* we emulate some simple devices */
			if (kr->io.port >= 0x40 && kr->io.port < 0x50) {
				emulate_hd(kr);
			} else if (kr->io.port == 0x60) {
				emulate_serial(kr);
			} else if (kr->io.port == 0x70) {
				foo(kr);
			} else {
				printf("I/O: size %d count %d port %x dir: %s\n",
						kr->io.size,
						kr->io.count,
						kr->io.port,
						kr->io.direction == KVM_EXIT_IO_OUT ?
							"out" : "in");
			}
			break;
		case KVM_EXIT_HLT:
			running = false;
			break;
		default:
			cerr << "Exit reason: " << kr->exit_reason << endl;
			running = false;
		}
	}
	return 0;
}

/* load_elf(): create the initial state of the Virtual Machine by initializing its memory
 * and its CPU. The machine starts with the ELF program loaded in memory and the
 * CPU ready to jump to the ELF entry point.
 *
 * The guest CPU will start in non-root/system mode.
 */

/* definitions for control register bits
 * we define all the bits for completeness, but we don't need most of them
 */

/* CR0 bits */
static unsigned int const CR0_PE = 1U << 0;		// protection enable
static unsigned int const CR0_MP = 1U << 1;		// FPU (sync with main proc)
static unsigned int const CR0_EM = 1U << 2;		// FPU (present or not)
static unsigned int const CR0_TS = 1U << 3;		// FPU (save FPU ctx on task switch)
static unsigned int const CR0_ET = 1U << 4;		// FPU (model type)
static unsigned int const CR0_NE = 1U << 5;		// FPU (enable error reporting)
static unsigned int const CR0_WP = 1U << 16;		// write-protect (enforce R/W for system too)
static unsigned int const CR0_AM = 1U << 18;		// alignment check
static unsigned int const CR0_NW = 1U << 29;		// non-write through
static unsigned int const CR0_CD = 1U << 30;		// cache disable
static unsigned int const CR0_PG = 1U << 31;		// enable paging

/* CR4 bits */
static unsigned int const CR4_VME = 1U << 0;		// virtual-8086 enable
static unsigned int const CR4_PVI = 1U << 1;		// virtual interrupts
static unsigned int const CR4_TSD = 1U << 2;		// enable rdtsc for users
static unsigned int const CR4_DE = 1U << 3;		// enable debug breakpoints on I/O
static unsigned int const CR4_PSE = 1U << 4;		// page-size-extension (ignored in 64b)
static unsigned int const CR4_PAE = 1U << 5;		// physical-address-extension (required for 64b)
static unsigned int const CR4_MCE = 1U << 6;		// enables machine check exceptions
static unsigned int const CR4_PGE = 1U << 7;		// enables th Global bit in paget table entries
static unsigned int const CR4_PCE = 1U << 8;		// enables the performance monitor counters
static unsigned int const CR4_OSFXSR = 1U << 8;		// enables SSE instructions
static unsigned int const CR4_OSXMMEXCPT = 1U << 10;	// enables unmasked SSE exceptions
static unsigned int const CR4_UMIP = 1U << 11;		// forbids SGDT, SIDT, SLDT, SMSW and STR in usermode
static unsigned int const CR4_VMXE = 1U << 13;		// enables virtual machine extensions
static unsigned int const CR4_SMXE = 1U << 14;		// enables Trusted Execution Technology
static unsigned int const CR4_FSGSBASE = 1U << 16;	// enables {RD,WR}{FS,GS}BASE instructions
static unsigned int const CR4_PCIDE = 1U << 17;		// enables Process Context Identifiers in TLBs
static unsigned int const CR4_OSXSAVE = 1U << 18;	// enables XSAVE
static unsigned int const CR4_SMEP = 1U << 20;		// enables SMEP
static unsigned int const CR4_SMAP = 1U << 21;		// enables SMAP
static unsigned int const CR4_PKE  = 1U << 22;		// enables Protection Keys

/* EFER bits = EFER is Model Specific Register number 0xC0000080; */
static unsigned int const EFER_SCE = 1U << 0;		// system call extensions (enables syscall/sysret)
static unsigned int const EFER_LME = 1U << 8;		// long mode enable
static unsigned int const EFER_LMA = 1U << 10;		// long mode active
static unsigned int const EFER_NXE = 1U << 11;		// enables the NX bit in page tables
static unsigned int const EFER_SVME = 1U << 12;		// enables Secure Virtual Machine (AMD VM extensions)
static unsigned int const EFER_LMSLE = 1U << 13;	// enables Long Mode Segment Limit
static unsigned int const EFER_FFXSR = 1U << 14;	// enables fast FXSAVE/FXRSTOR
static unsigned int const EFER_TCE = 1U << 15;		// enables Translaction Cache Extensions
void foo_init();
bool load_elf(int vcpu_fd, const char *fname)
{
	/* helper structs for autmatically releasing resources on function return */
	struct fdcloser {
		int fd;
		fdcloser(int fd_): fd(fd_) {}
		~fdcloser() { close(fd); }
	};
	struct munmapper {
		void *m;
		size_t s;
		munmapper(void *m_, size_t s_): m(m_), s(s_) {}
		~munmapper() { munmap(m, s); }
	};


	/* we open and read the ELF file, then load it into the guest memory */
	int fd = open(fname, O_RDONLY);
	if (fd < 0) {
		cerr << fname << ": " << strerror(errno) << endl;
		return false;
	}
	fdcloser fd_c(fd);

	/* parsing ELF files is much easier if they are mapped in memory.  For
	 * this we need the file size, which we can obtain wit stat().
	 */
	struct stat st;
	if (fstat(fd, &st) < 0) {
		cerr << fname << ": " << strerror(errno) << endl;
		return false;
	}

	/* map the whole file in memory, so that we can simply add ELF offsets to 'm' */
	unsigned char *m = static_cast<unsigned char*>(mmap(0, st.st_size, PROT_READ, MAP_PRIVATE, fd, 0));
	if (m == MAP_FAILED) {
		cerr << fname << ": " << strerror(errno) << endl;
		return false;
	}
	munmapper m_u(m, st.st_size);

	/* the ELF header should now be found at the start of the mapped memory */
	Elf64_Ehdr *eh = reinterpret_cast<Elf64_Ehdr*>(m);

	/* simple sanity checks */
	static const unsigned char ident[] = { ELFMAG0, ELFMAG1, ELFMAG2, ELFMAG3, ELFCLASS64, 
		ELFDATA2LSB, EV_CURRENT };
	for (int i = 0; i < sizeof(ident); i++) {
		if (eh->e_ident[i] != ident[i]) {
			cerr << "Unexpected ident byte at " << i << ": got " << eh->e_ident[i] <<
				", want " << ident[i] << endl;
			return false;
		}
	}

	/* the guest memory will contain the segments copied from the ELF file, but we also
	 * need a few page tables to enable 64bit long mode, and a stack. We place the tables
	 * towards the end of the guest memory and the stack immediatly before them.
	 */

	/* we identity-map 1GiB using 1GiB pages */
	uint64_t tab4_addr = GUESTMEMSZ - 4096;
	uint64_t *tab4 = reinterpret_cast<uint64_t *>(&guestmem[tab4_addr]);
	uint64_t tab3_addr = GUESTMEMSZ - 2*4096;
	uint64_t *tab3 = reinterpret_cast<uint64_t *>(&guestmem[tab3_addr]);
	uint64_t gueststack = GUESTMEMSZ - 2*4096;

	tab4[0] = tab3_addr | 0x3; // present, R/W
	tab3[0] = 0 | 0x83; // present, R/W, PS

	/* now we load the ELF segments, checking that we only read from the mapped file and
	 * write into the guest memory.
	 * We don't care about overwriting the stack or the page tables, though, since that 
	 * only affects the guest.
	 * */
	uint64_t phentoff = 0,			// offset of the current program table entry
		 newphentoff = eh->e_phoff;	// offset of the next program table entry
	for (unsigned int i = 0; i < eh->e_phnum; i++) {

		/* defend against specially crafted ELF files */
		if (newphentoff < phentoff || newphentoff >= st.st_size) {
			cerr << "malformed ELF file" << endl;
			return false;
		}

		phentoff = newphentoff;
		Elf64_Phdr *ph = reinterpret_cast<Elf64_Phdr*>(m + phentoff);
		
		if (ph->p_type != PT_LOAD)
			continue;

		/* check for non-sensical entry fields */
		if (ph->p_offset >= st.st_size ||
		    ph->p_memsz < ph->p_filesz ||
		    ph->p_offset + ph->p_filesz < ph->p_offset ||
		    ph->p_offset + ph->p_filesz > st.st_size) {
			cerr << "malformed section " << i << endl;
			return false;
		}

		/* the segment must be loaded into [ph->p_vaddr, ph->p_vaddr + ph->p_memsz).
		 * Check that the interval is well formed and fits into the guest memory.
		 *
		 * This is not necessarily an error in the ELF file: the program may be
		 * simply too big for our allocated guest memory.
		 */
		if (ph->p_vaddr >= GUESTMEMSZ ||
		    ph->p_vaddr + ph->p_memsz < ph->p_vaddr ||
		    ph->p_vaddr + ph->p_memsz > GUESTMEMSZ) {
			cerr << "section " << i << "out of bounds" << endl;
			return false;
		}

		/* now we can safely copy the segment */
		memcpy(guestmem + ph->p_vaddr, m + ph->p_offset, ph->p_filesz);
		/* if p_memsz is larger than p_filesz, the additional bytes must be zeroed */
		memset(guestmem + ph->p_vaddr + ph->p_filesz, 0, ph->p_memsz - ph->p_filesz);

		newphentoff = phentoff + eh->e_phentsize;
	}

	/* now we initialize the CPU. For 64b long mode we need to properly
	 * initialize the CS segment selector the CR0, CR3 and CR4 registers,
	 * and the EFER Model Specific Register. CR3 must point to the level 4
	 * page table that we have created into the guest memory.
	 *
	 * Note that we can skip the creation of the GDT, since we are loading
	 * the CS selector directly. We also skip most interrupt-related data
	 * structures, since we start with interrupts disabled. The guest can
	 * create these data structures by itself if it needs them.
	 */
	kvm_sregs sregs;
	memset(&sregs, 0, sizeof(sregs));

	/* create a code segment (we only set the fields that are meaningful in
	 * 64b and/or are checked on VM entry)
	 */
	// checked on VM entry
	sregs.cs.present = 1;
	sregs.cs.type = 0xa;		// code, readeable
	sregs.cs.s = 1;			// reserved (must be 1)
	// meaningful in 64b
	sregs.cs.dpl = 0;		// descriptor privilege level
	sregs.cs.db = 0;		// default operand size (must be 0)
	sregs.cs.l = 1;			// long mode
	// we also need to set at least the P and type fields of the TR register
	sregs.tr.present = 1;
	sregs.tr.type = 0xa;

	/* set the control registers according to AMD64 documentation for 64b mode:
	 * - enable protection and paging in cr0
	 * - enable physical address extensions in cr4
	 * - enable long mode in EFER
	 * kvm also checks that we set LMA in a consistent state in EFER.
	 */
	sregs.cr0 = CR0_PE | CR0_PG;
	sregs.cr3 = tab4_addr;
	sregs.cr4 = CR4_PAE;
	sregs.efer = EFER_LME | EFER_LMA;

	if (ioctl(vcpu_fd, KVM_SET_SREGS, &sregs) < 0) {
		cerr << "set sregs: " << strerror(errno) << endl;
		return 1;
	}

	/* now we initalize the normal registers. We can set them all at zero,
	 * except for rip and rsp.
	 */
	struct kvm_regs regs;
	memset(&regs, 0, sizeof(regs));
	regs.rip = eh->e_entry;
	regs.rsp = gueststack;

	if (ioctl(vcpu_fd, KVM_SET_REGS, &regs) < 0) {
		cerr << "set regs: " << strerror(errno) << endl;
		return 1;
	}

	// initialize the "foo" device below
	foo_init();

	return true;
}

/* emulation of the useless "foo" device */
void* checkaddr(unsigned int a)
{
	if (a >= GUESTMEMSZ) {
		return nullptr;
	}
	return &guestmem[a];
}

void foo_log(void *a)
{
	cout << "address " << a << endl;
}

void (*foo_ptr)(void *a);

void foo_init()
{
	foo_ptr = foo_log;
}

void foo(kvm_run *kr)
{
	char *p = reinterpret_cast<char*>(kr) + kr->io.data_offset;
	unsigned int *ioparam = reinterpret_cast<unsigned int*>(p);

	void* a = checkaddr(*ioparam);
	(*foo_ptr)(a);
}

/* emulation of a very simple DMA-capable hard disk. The hard disk
 * contains NUM_SECT sectors and can copy any sector to and from guest
 * memory
 */
static int const NUM_SECT = 100;
struct hd {
	unsigned char sectors[512][NUM_SECT];

	unsigned int snr;	// 0x40, sector number
	unsigned int mem;	// 0x44, guest memory address
	unsigned int cmd;	// 0x48, 1: DMA-in, 2: DMA-out
} hda;

void emulate_hd(kvm_run *kr)
{
	char *p = reinterpret_cast<char*>(kr) + kr->io.data_offset;
	unsigned int *ioparam = reinterpret_cast<unsigned int*>(p);

	if (kr->io.direction == KVM_EXIT_IO_OUT) {
		switch (kr->io.port) {
		case 0x40:
			if (*ioparam < 100) {
				hda.snr = *ioparam;
			}	
			break;
		case 0x44:
			hda.mem = *ioparam;
			break;
		case 0x48:
			if (*ioparam == 1) {
				memcpy(&guestmem[hda.mem], &hda.sectors[0][hda.snr], 512);
			} else if (*ioparam == 2) {
				memcpy(&hda.sectors[0][hda.snr], &guestmem[hda.mem], 512);
			}
			break;
		}
	} else {
		switch (kr->io.port) {
		case 0x40:
			*ioparam = hda.snr;
			break;
		case 0x44:
			*ioparam = hda.mem;
			break;
		case 0x48:
			*ioparam = hda.cmd;
			break;
		}
	}
}

/* emulate a very simple serial port. There is just ony register
 * and bytes written into it are sent to the VMM standard output.
 */
void emulate_serial(kvm_run *kr)
{
	if (kr->io.direction == KVM_EXIT_IO_OUT) {
		char *p = reinterpret_cast<char*>(kr) + kr->io.data_offset;

		cout << *p;
	}
}
